{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DeenReinierCrusem/CPEN65-1/blob/main/Laboratory_Exercise_8_Training_Deep_Neural_Networks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Laboratory Exercise 8: Training Deep Neural Networks"
      ],
      "metadata": {
        "id": "mnySLXRZgXhx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VfWvy3t82RyY",
        "outputId": "1fe91e99-7535-49cf-8948-a137e3c0e635"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "765/765 [==============================] - 6s 6ms/step - loss: 0.1078 - accuracy: 0.9669 - val_loss: 0.0832 - val_accuracy: 0.9747\n",
            "Epoch 2/100\n",
            "765/765 [==============================] - 3s 4ms/step - loss: 0.0573 - accuracy: 0.9820 - val_loss: 0.0497 - val_accuracy: 0.9824\n",
            "Epoch 3/100\n",
            "765/765 [==============================] - 3s 5ms/step - loss: 0.0417 - accuracy: 0.9873 - val_loss: 0.0567 - val_accuracy: 0.9820\n",
            "Epoch 4/100\n",
            "765/765 [==============================] - 3s 4ms/step - loss: 0.0282 - accuracy: 0.9910 - val_loss: 0.0695 - val_accuracy: 0.9825\n",
            "Epoch 5/100\n",
            "765/765 [==============================] - 5s 6ms/step - loss: 0.0300 - accuracy: 0.9902 - val_loss: 0.0539 - val_accuracy: 0.9853\n",
            "Epoch 6/100\n",
            "765/765 [==============================] - 3s 4ms/step - loss: 0.0234 - accuracy: 0.9931 - val_loss: 0.0487 - val_accuracy: 0.9879\n",
            "Epoch 7/100\n",
            "765/765 [==============================] - 3s 4ms/step - loss: 0.0183 - accuracy: 0.9939 - val_loss: 0.0742 - val_accuracy: 0.9846\n",
            "Epoch 8/100\n",
            "765/765 [==============================] - 4s 5ms/step - loss: 0.0242 - accuracy: 0.9919 - val_loss: 0.0667 - val_accuracy: 0.9827\n",
            "Epoch 9/100\n",
            "765/765 [==============================] - 4s 5ms/step - loss: 0.0162 - accuracy: 0.9948 - val_loss: 0.0655 - val_accuracy: 0.9838\n",
            "Epoch 10/100\n",
            "765/765 [==============================] - 3s 5ms/step - loss: 0.0141 - accuracy: 0.9961 - val_loss: 0.0565 - val_accuracy: 0.9887\n",
            "Epoch 11/100\n",
            "765/765 [==============================] - 3s 4ms/step - loss: 0.0112 - accuracy: 0.9963 - val_loss: 0.0444 - val_accuracy: 0.9908\n",
            "Epoch 12/100\n",
            "765/765 [==============================] - 4s 6ms/step - loss: 0.0144 - accuracy: 0.9959 - val_loss: 0.0628 - val_accuracy: 0.9877\n",
            "Epoch 13/100\n",
            "765/765 [==============================] - 3s 4ms/step - loss: 0.0108 - accuracy: 0.9967 - val_loss: 0.0684 - val_accuracy: 0.9871\n",
            "Epoch 14/100\n",
            "765/765 [==============================] - 3s 4ms/step - loss: 0.0126 - accuracy: 0.9963 - val_loss: 0.0603 - val_accuracy: 0.9874\n",
            "Epoch 15/100\n",
            "765/765 [==============================] - 3s 4ms/step - loss: 0.0086 - accuracy: 0.9973 - val_loss: 0.0885 - val_accuracy: 0.9846\n",
            "Epoch 16/100\n",
            "765/765 [==============================] - 4s 5ms/step - loss: 0.0091 - accuracy: 0.9972 - val_loss: 0.0506 - val_accuracy: 0.9891\n",
            "Epoch 17/100\n",
            "765/765 [==============================] - 3s 5ms/step - loss: 0.0076 - accuracy: 0.9977 - val_loss: 0.0876 - val_accuracy: 0.9850\n",
            "Epoch 18/100\n",
            "765/765 [==============================] - 3s 5ms/step - loss: 0.0154 - accuracy: 0.9962 - val_loss: 0.0641 - val_accuracy: 0.9868\n",
            "Epoch 19/100\n",
            "765/765 [==============================] - 4s 6ms/step - loss: 0.0103 - accuracy: 0.9969 - val_loss: 0.0627 - val_accuracy: 0.9892\n",
            "Epoch 20/100\n",
            "765/765 [==============================] - 3s 4ms/step - loss: 0.0043 - accuracy: 0.9985 - val_loss: 0.0605 - val_accuracy: 0.9895\n",
            "Epoch 21/100\n",
            "765/765 [==============================] - 3s 4ms/step - loss: 0.0080 - accuracy: 0.9976 - val_loss: 0.1023 - val_accuracy: 0.9788\n",
            "161/161 [==============================] - 0s 2ms/step - loss: 0.0319 - accuracy: 0.9932\n",
            "Test loss: 0.03186243027448654\n",
            "Test accuracy: 0.9931893348693848\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load the MNIST dataset\n",
        "mnist = keras.datasets.mnist\n",
        "(X_train_full, y_train_full), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Filter the dataset to keep only digits 0 to 4\n",
        "idx_train = (y_train_full <= 4)\n",
        "idx_test = (y_test <= 4)\n",
        "X_train, y_train = X_train_full[idx_train], y_train_full[idx_train]\n",
        "X_test, y_test = X_test[idx_test], y_test[idx_test]\n",
        "\n",
        "# Normalize pixel values to the range [0, 1]\n",
        "X_train = X_train / 255.0\n",
        "X_test = X_test / 255.0\n",
        "\n",
        "# Split the training set into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the deep neural network (DNN) architecture\n",
        "model = keras.models.Sequential([\n",
        "    keras.layers.Flatten(input_shape=[28, 28]),\n",
        "    keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
        "    keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
        "    keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
        "    keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
        "    keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
        "    keras.layers.Dense(5, activation=\"softmax\")\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "# Define callbacks for early stopping and checkpoint saving\n",
        "early_stopping_cb = keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)\n",
        "checkpoint_cb = keras.callbacks.ModelCheckpoint(\"mnist_dnn_model.h5\", save_best_only=True)\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train, y_train, epochs=100, validation_data=(X_val, y_val),\n",
        "                    callbacks=[early_stopping_cb, checkpoint_cb])\n",
        "\n",
        "# Load the best model saved during training\n",
        "model = keras.models.load_model(\"mnist_dnn_model.h5\")\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
        "print(\"Test loss:\", test_loss)\n",
        "print(\"Test accuracy:\", test_accuracy)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RDmQyFKu3AOq",
        "outputId": "10c0e0fb-3c4f-4614-dee6-b1e1140e8af6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 1s 0us/step\n",
            "Epoch 1/100\n",
            "765/765 [==============================] - 5s 5ms/step - loss: 0.1156 - accuracy: 0.9636 - val_loss: 0.0582 - val_accuracy: 0.9819\n",
            "Epoch 2/100\n",
            "765/765 [==============================] - 3s 4ms/step - loss: 0.0530 - accuracy: 0.9840 - val_loss: 0.0506 - val_accuracy: 0.9838\n",
            "Epoch 3/100\n",
            "765/765 [==============================] - 4s 5ms/step - loss: 0.0367 - accuracy: 0.9890 - val_loss: 0.0563 - val_accuracy: 0.9845\n",
            "Epoch 4/100\n",
            "765/765 [==============================] - 4s 5ms/step - loss: 0.0328 - accuracy: 0.9896 - val_loss: 0.0698 - val_accuracy: 0.9830\n",
            "Epoch 5/100\n",
            "765/765 [==============================] - 3s 4ms/step - loss: 0.0248 - accuracy: 0.9924 - val_loss: 0.0475 - val_accuracy: 0.9886\n",
            "Epoch 6/100\n",
            "765/765 [==============================] - 3s 4ms/step - loss: 0.0233 - accuracy: 0.9929 - val_loss: 0.0750 - val_accuracy: 0.9828\n",
            "Epoch 7/100\n",
            "765/765 [==============================] - 4s 6ms/step - loss: 0.0207 - accuracy: 0.9939 - val_loss: 0.0406 - val_accuracy: 0.9899\n",
            "Epoch 8/100\n",
            "765/765 [==============================] - 3s 4ms/step - loss: 0.0164 - accuracy: 0.9950 - val_loss: 0.0536 - val_accuracy: 0.9876\n",
            "Epoch 9/100\n",
            "765/765 [==============================] - 3s 4ms/step - loss: 0.0182 - accuracy: 0.9946 - val_loss: 0.0486 - val_accuracy: 0.9874\n",
            "Epoch 10/100\n",
            "765/765 [==============================] - 3s 4ms/step - loss: 0.0136 - accuracy: 0.9960 - val_loss: 0.0670 - val_accuracy: 0.9850\n",
            "Epoch 11/100\n",
            "765/765 [==============================] - 4s 6ms/step - loss: 0.0107 - accuracy: 0.9964 - val_loss: 0.0521 - val_accuracy: 0.9887\n",
            "Epoch 12/100\n",
            "765/765 [==============================] - 3s 4ms/step - loss: 0.0112 - accuracy: 0.9969 - val_loss: 0.0438 - val_accuracy: 0.9908\n",
            "Epoch 13/100\n",
            "765/765 [==============================] - 3s 4ms/step - loss: 0.0113 - accuracy: 0.9967 - val_loss: 0.0576 - val_accuracy: 0.9882\n",
            "Epoch 14/100\n",
            "765/765 [==============================] - 4s 5ms/step - loss: 0.0120 - accuracy: 0.9966 - val_loss: 0.0447 - val_accuracy: 0.9917\n",
            "Epoch 15/100\n",
            "765/765 [==============================] - 4s 5ms/step - loss: 0.0100 - accuracy: 0.9969 - val_loss: 0.0748 - val_accuracy: 0.9848\n",
            "Epoch 16/100\n",
            "765/765 [==============================] - 3s 4ms/step - loss: 0.0107 - accuracy: 0.9969 - val_loss: 0.0518 - val_accuracy: 0.9884\n",
            "Epoch 17/100\n",
            "765/765 [==============================] - 4s 5ms/step - loss: 0.0086 - accuracy: 0.9974 - val_loss: 0.0366 - val_accuracy: 0.9912\n",
            "Epoch 18/100\n",
            "765/765 [==============================] - 5s 6ms/step - loss: 0.0096 - accuracy: 0.9976 - val_loss: 0.0425 - val_accuracy: 0.9907\n",
            "Epoch 19/100\n",
            "765/765 [==============================] - 3s 4ms/step - loss: 0.0059 - accuracy: 0.9983 - val_loss: 0.0544 - val_accuracy: 0.9908\n",
            "Epoch 20/100\n",
            "765/765 [==============================] - 3s 4ms/step - loss: 0.0079 - accuracy: 0.9978 - val_loss: 0.0684 - val_accuracy: 0.9894\n",
            "Epoch 21/100\n",
            "765/765 [==============================] - 4s 5ms/step - loss: 0.0085 - accuracy: 0.9978 - val_loss: 0.0439 - val_accuracy: 0.9917\n",
            "Epoch 22/100\n",
            "765/765 [==============================] - 4s 5ms/step - loss: 0.0059 - accuracy: 0.9984 - val_loss: 0.0443 - val_accuracy: 0.9908\n",
            "Epoch 23/100\n",
            "765/765 [==============================] - 3s 5ms/step - loss: 0.0055 - accuracy: 0.9984 - val_loss: 0.0846 - val_accuracy: 0.9861\n",
            "Epoch 24/100\n",
            "765/765 [==============================] - 3s 5ms/step - loss: 0.0095 - accuracy: 0.9977 - val_loss: 0.0418 - val_accuracy: 0.9917\n",
            "Epoch 25/100\n",
            "765/765 [==============================] - 5s 6ms/step - loss: 0.0059 - accuracy: 0.9987 - val_loss: 0.0611 - val_accuracy: 0.9886\n",
            "Epoch 26/100\n",
            "765/765 [==============================] - 3s 5ms/step - loss: 0.0075 - accuracy: 0.9981 - val_loss: 0.0498 - val_accuracy: 0.9902\n",
            "Epoch 27/100\n",
            "765/765 [==============================] - 4s 5ms/step - loss: 0.0080 - accuracy: 0.9978 - val_loss: 0.0769 - val_accuracy: 0.9874\n",
            "161/161 [==============================] - 0s 2ms/step - loss: 0.0268 - accuracy: 0.9944\n",
            "Test loss: 0.026755807921290398\n",
            "Test accuracy: 0.9943568706512451\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load MNIST dataset\n",
        "mnist = keras.datasets.mnist\n",
        "(X_train_full, y_train_full), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Keep only digits 0 to 4\n",
        "idx_train = (y_train_full <= 4)\n",
        "idx_test = (y_test <= 4)\n",
        "X_train, y_train = X_train_full[idx_train], y_train_full[idx_train]\n",
        "X_test, y_test = X_test[idx_test], y_test[idx_test]\n",
        "\n",
        "# Normalize pixel values\n",
        "X_train = X_train / 255.0\n",
        "X_test = X_test / 255.0\n",
        "\n",
        "# Split training set into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the DNN architecture\n",
        "model = keras.models.Sequential([\n",
        "    keras.layers.Flatten(input_shape=[28, 28]),\n",
        "    keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
        "    keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
        "    keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
        "    keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
        "    keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
        "    keras.layers.Dense(5, activation=\"softmax\")\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "# Define callbacks for early stopping and checkpoint saving\n",
        "early_stopping_cb = keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)\n",
        "checkpoint_cb = keras.callbacks.ModelCheckpoint(\"mnist_dnn_model.h5\", save_best_only=True)\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train, y_train, epochs=100, validation_data=(X_val, y_val),\n",
        "                    callbacks=[early_stopping_cb, checkpoint_cb])\n",
        "\n",
        "# Load the best model saved during training\n",
        "model = keras.models.load_model(\"mnist_dnn_model.h5\")\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
        "print(\"Test loss:\", test_loss)\n",
        "print(\"Test accuracy:\", test_accuracy)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9YJiEcrY4RxG"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "\n",
        "# Define DNN A\n",
        "dnn_a = keras.models.Sequential([\n",
        "    keras.layers.Flatten(input_shape=[28, 28]),\n",
        "    keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
        "    keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
        "    keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
        "    keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
        "    keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\")\n",
        "])\n",
        "\n",
        "# Define DNN B\n",
        "dnn_b = keras.models.Sequential([\n",
        "    keras.layers.Flatten(input_shape=[28, 28]),\n",
        "    keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
        "    keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
        "    keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
        "    keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
        "    keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\")\n",
        "])\n",
        "\n",
        "# Concatenate the outputs of DNN A and DNN B\n",
        "concat_layer = keras.layers.Concatenate()([dnn_a.output, dnn_b.output])\n",
        "\n",
        "# Add an additional hidden layer\n",
        "hidden_layer = keras.layers.Dense(10, activation=\"relu\")(concat_layer)\n",
        "\n",
        "# Add the output layer with a single neuron\n",
        "output_layer = keras.layers.Dense(1, activation=\"sigmoid\")(hidden_layer)\n",
        "\n",
        "# Create the final model\n",
        "model = keras.models.Model(inputs=[dnn_a.input, dnn_b.input], outputs=output_layer)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def generate_training_batch(X_train, y_train, batch_size):\n",
        "    num_classes = len(np.unique(y_train))\n",
        "    half_batch_size = batch_size // 2\n",
        "\n",
        "    # Generate pairs of images from the same class\n",
        "    class_indices = np.random.randint(0, num_classes, half_batch_size)\n",
        "    class_indices = np.repeat(class_indices, 2)\n",
        "    same_class_images = []\n",
        "    for class_index in class_indices:\n",
        "        class_indices = np.where(y_train == class_index)[0]\n",
        "        pair_indices = np.random.choice(class_indices, size=2, replace=False)\n",
        "        same_class_images.append(pair_indices)\n",
        "    same_class_images = np.array(same_class_images)\n",
        "    same_class_labels = np.zeros(half_batch_size)\n",
        "\n",
        "    # Generate pairs of images from different classes\n",
        "    diff_class_images = []\n",
        "    for _ in range(half_batch_size):\n",
        "        class_indices = np.random.choice(num_classes, size=2, replace=False)\n",
        "        pair_indices = []\n",
        "        for class_index in class_indices:\n",
        "            class_indices = np.where(y_train == class_index)[0]\n",
        "            image_index = np.random.choice(class_indices)\n",
        "            pair_indices.append(image_index)\n",
        "        diff_class_images.append(pair_indices)\n",
        "    diff_class_images = np.array(diff_class_images)\n",
        "    diff_class_labels = np.ones(half_batch_size)\n",
        "\n",
        "    # Concatenate same-class and different-class images and labels\n",
        "    images = np.concatenate([X_train[same_class_images[:, 0]], X_train[diff_class_images[:, 0]]], axis=0)\n",
        "    images2 = np.concatenate([X_train[same_class_images[:, 1]], X_train[diff_class_images[:, 1]]], axis=0)\n",
        "    labels = np.concatenate([same_class_labels, diff_class_labels], axis=0)\n",
        "\n",
        "    return [images, images2], labels\n"
      ],
      "metadata": {
        "id": "Vc40Br0p46BS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "\n",
        "# Load MNIST dataset\n",
        "mnist = keras.datasets.mnist\n",
        "(X_train_full, y_train_full), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Preprocess the dataset\n",
        "X_train_full = X_train_full.astype('float32') / 255.0\n",
        "X_test = X_test.astype('float32') / 255.0\n",
        "\n",
        "# Split MNIST training set into split #1 (55,000 images) and split #2 (5,000 images)\n",
        "X_train_split1, X_train_split2 = X_train_full[:55000], X_train_full[55000:]\n",
        "y_train_split1, y_train_split2 = y_train_full[:55000], y_train_full[55000:]\n",
        "\n",
        "# Rest of your code...\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jJ8exHKn49fd",
        "outputId": "196a84f9-0f5c-4745-a4c9-8127fb64b458"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Freeze the hidden layers of DNN A\n",
        "for layer in dnn_a.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Add a softmax output layer with 10 neurons on top of the frozen layers\n",
        "new_output_layer = keras.layers.Dense(10, activation=\"softmax\")(dnn_a.layers[-1].output)\n",
        "\n",
        "# Create the new model for split #2 training\n",
        "new_model = keras.models.Model(inputs=dnn_a.input, outputs=new_output_layer)\n"
      ],
      "metadata": {
        "id": "WEymTc-B5Qel"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the new model\n",
        "new_model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "# Train the new model on split #2\n",
        "new_model.fit(X_train_split2, y_train_split2, epochs=10, batch_size=32)\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "test_loss, test_accuracy = new_model.evaluate(X_test, y_test)\n",
        "print(\"Test loss:\", test_loss)\n",
        "print(\"Test accuracy:\", test_accuracy)\n"
      ],
      "metadata": {
        "id": "LLMEcWKu5Wvj",
        "outputId": "329800af-84bd-4304-86e2-ea81298be17f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "157/157 [==============================] - 1s 3ms/step - loss: 2.0222 - accuracy: 0.3380\n",
            "Epoch 2/10\n",
            "157/157 [==============================] - 0s 3ms/step - loss: 1.2845 - accuracy: 0.6250\n",
            "Epoch 3/10\n",
            "157/157 [==============================] - 1s 4ms/step - loss: 1.0389 - accuracy: 0.6962\n",
            "Epoch 4/10\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.9176 - accuracy: 0.7290\n",
            "Epoch 5/10\n",
            "157/157 [==============================] - 1s 4ms/step - loss: 0.8418 - accuracy: 0.7514\n",
            "Epoch 6/10\n",
            "157/157 [==============================] - 1s 4ms/step - loss: 0.7899 - accuracy: 0.7642\n",
            "Epoch 7/10\n",
            "157/157 [==============================] - 0s 3ms/step - loss: 0.7516 - accuracy: 0.7726\n",
            "Epoch 8/10\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 0.7210 - accuracy: 0.7824\n",
            "Epoch 9/10\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 0.6974 - accuracy: 0.7878\n",
            "Epoch 10/10\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 0.6772 - accuracy: 0.7944\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.7867 - accuracy: 0.7491\n",
            "Test loss: 0.7867380380630493\n",
            "Test accuracy: 0.7491000294685364\n"
          ]
        }
      ]
    }
  ]
}